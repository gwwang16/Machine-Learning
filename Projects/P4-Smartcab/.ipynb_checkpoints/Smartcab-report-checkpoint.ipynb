{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Reinforcement Learning\n",
    "## Project 4: Smartcab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smartcab is a self-driving car from the not-so-distant future that ferries people from one arbitrary location to another. In this project, you will use reinforcement learning to train a smartcab how to drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a basic driving agent\n",
    "Implement the basic driving agent, which processes the following inputs at each time step:\n",
    "- Next waypoint location, relative to its current location and heading,\n",
    "- Intersection state (traffic light and presence of cars), and,\n",
    "- Current deadline value (time steps remaining),\n",
    "- Produces some random move/action (`None`, `forward`, `left`, `right`). \n",
    "\n",
    "Don’t try to implement the correct strategy! That’s exactly what your agent is supposed to learn.\n",
    "\n",
    "Run this agent within the simulation\n",
    "environment with enforce_deadline set to False (see run function in agent.py), and observe how it performs. In this mode, the agent is given unlimited time to reach the destination. The current state, action taken by your agent and reward/penalty earned are shown in the simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "*In your report, mention what you see in the agent’s behavior. Does it eventually make it to the target location?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  The agent moved randomly in each direction if I added random actions for the agent. It would reach the target location in the last sometimes, but the deadline was exceeded almost every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and update state\n",
    "Identify a set of states that you think are appropriate for modeling the driving agent. The main source of state variables are current inputs, but not all of them may be worth representing. Also, you can choose to explicitly define states, or use some combination (vector) of inputs as an implicit state.\n",
    "At each time step, process the inputs and update the current state. Run it again (and as often as you need) to observe how the reported state changes through the run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "*Justify why you picked these set of states, and how they model the agent and its environment.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "\n",
    "The states I used are:\n",
    "- The traffic light `light`: ['red', 'green']\n",
    "- Next waypoint location `next_waypoint`: [None, 'forward', 'left', 'right']\n",
    "\n",
    "The goal of this project is to train the cab can reach the target and obey the traffic rules. \n",
    "\n",
    "The `light`, `oncoming`, `left`, and `right` states are related to traffic rules, the agent will get reward or penalty according to whether it obeyed the traffic rules. The traffic rules are:\n",
    "- `light`: ['red'], the agent can turn right if `oncoming` state is not ['left'] or `left` state is not ['forward']. \n",
    "- `light`: ['green'], the agent can turn left only if `oncoming` state is not ['forward'] .\n",
    "\n",
    "However, when I used all these states, the number of disobeying traffic rules was high. So I reduced the states number,  the `light` state is used only in the last to learn the traffic rule. \n",
    "\n",
    "`next_waypoint` state is the best action calculated by route planner, the agent should obey the action from `nex_waypoint` as well to reach the target point as soon as possible.\n",
    "\n",
    "`deadline` is another state related with the reward. The agent can not get the final reward 10 if it can not reach the target within deadline time. However, this state will affact the performance when I added it into the set of sates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Q-Learning\n",
    "Implement the Q-Learning algorithm by initializing and updating a table/mapping of Q-values at each time step. Now, instead of randomly selecting an action, pick the best action available from the current state based on Q-values, and return that.\n",
    "Each action generates a corresponding numeric reward or penalty (which may be zero). Your agent should take this into account when updating Q-values. Run it again, and observe the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "*What changes do you notice in the agent’s behavior?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The agent moved randomly firstly, and then it obeyed the traffic rule, and followed the actions provided by `next_waypoint` state in the last.\n",
    "\n",
    "However, the agent can not reach the target within deadline time and won't obey the derection provided by route planner sometimes befor some parameters tuned(as shown in next question)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance the driving agent\n",
    "Apply the reinforcement learning techniques you have learnt, and tweak the parameters (e.g. learning rate, discount factor, action selection method, etc.), to improve the performance of your agent. Your goal is to get it to a point so that within 100 trials, the agent is able to learn a feasible policy - i.e. reach the destination within the allotted time, with net reward remaining positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "*Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "- Action selection method is picking the available action with max Q-value as the best action from the current state based on Q-values.\n",
    "- Learning rate determines to what extent the newly acquired information will override the old information.\n",
    " - Learning rate is 0, it will make the agent not learn anything, the Q-values do not update, the success rate is about 0.\n",
    " - Learning rate is 1, there is a very high success rate, which is 98/100 ~ 100/100 from 10 trails.\n",
    "- Discount factor determines the importance of future rewards. \n",
    " - It closes to 0 leads to \"myopic\" evaluation, while close to 1 leads to \"far-sighted\" evaluation.\n",
    "\n",
    "\n",
    "When the states only contain `light` and `next_waypoint` states:\n",
    "The success rate is high and the traffic rule breaking times and planner action breaking times are very low. But when the learning rate less than 1, the success rate is not stable. \n",
    "\n",
    "When the states contain all the sates of `inputs` and `next_way_point`, the success rate is stable and very high. However, the raffic rule breaking times and planner action breaking times are quite high in the same time.\n",
    "<img src=\"https://raw.githubusercontent.com/wgw0415/Machine-Learning/master/Projects/P4-Smartcab/images/results.jpg\" width=\"400\">\n",
    "\n",
    "I wonder why the few states contained, the performance is better. So I increased the trail times from 100 to 1000, and found that:\n",
    " - when the states contained `light` and `next_waypoint`, the success rate is still high(999/1000), the traffic rule breaking time is increased rarely (the probability is about 0.3%), the reason is the agent did not know the overall traffic laws, it only concerns the red and green light, but not concerns whether there is traffic at the intersection. when I added extra 10 cars on the road, the probability of traffic rules breaking increased to 0.3%~2%, and the success rate is very unstale, the lowest success rate is about 30/100 among my trials. With the discount factor increasing, the success rate gets more stable.\n",
    " - when the states contained all `inputs` states and `next_waypoint`, the performance did not improved. The success rate is about 998/1000, The traffic rule and planner action breaking times were high and increased intermittently as well. The reason could be Q-values too widely dispersed due to the high dimensions.\n",
    " \n",
    "In this project, the learning rate should be chosen as 1 because it fully deterministic environment. \n",
    "To verify this conclusion, some experiments are conducted. The discount factor is 0.5, medium values are chosen among 5 trails. 10 extra cars are added in the evironment to make a more obvious difference.\n",
    "\n",
    "|Learning rate|Success rate| Disobeying traffic laws| Disobeying planner action|\n",
    "| :----------:|:----------:|:------:|:-----:|\n",
    "| 0           | 0/100      | 0      | 1654  |\n",
    "| 0.2         | 89/100     |  24    | 392   |\n",
    "| 0.4         | 40/100     | 10     |946    |\n",
    "| 0.6         | 88/100     | 12     |117    |\n",
    "| 0.8         |  98/100    | 8      |12     |\n",
    "| 1           |  99/100    | 7      |5      |\n",
    "\n",
    "We can found the performance is getting better with the increasing of learning rate.\n",
    "\n",
    "The discount factor should be selected a lower value to reduce the influence of high reward at the final target, because the agent can get the the target with `net_waypoint` action and the destination changed every time. To verify this conclusion, some experiments are conducted. The learning rate is 1, medium values are chosen among 5 trails. 10 extra cars are added in the evironment to make a more obvious difference.\n",
    "\n",
    "|Discount factor|Success rate|Disobeying trafficlaws|Disobeying planner action|\n",
    "| :----------:  |:----------:|:------:|:-----:|\n",
    "| 0             | 56/100     | 5      | 6     |\n",
    "| 0.2           | 67/100     | 5      | 6     |\n",
    "| 0.4           | 99/100     | 5      | 5     |\n",
    "| 0.6           | 100/100    | 5      | 7     |\n",
    "| 0.8           |  100/100   | 5      | 6     |\n",
    "| 1             |  100/100   | 5      | 6     |\n",
    "\n",
    "We can observe that the success rate increased obviously before the discount factor less than 0.4.\n",
    "\n",
    "I selected the `light` and `next_waypoint` as states, learning rate is 1. discount factor is 0.5 in the last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "*Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "1. My agent can get close to finding an optimal policy with a high success rate and lower faults number. But there is still few probability to make faults on traffic laws and the shortest path. In this project, the agent will disobeying traffic laws in the following conditions:\n",
    " - the light: ['red'], the agent turn right when `oncoming`:['left'] or `left`：['forward'];\n",
    " - the `light`:['green'], the agent turn left when `oncoming`:['forward']\n",
    "\n",
    "   The reason is the `light`:['red', 'green'] state included for traffic laws learning in training state. The agent will make mistake in these specific circumstances .\n",
    "\n",
    "2. It doed not reach the destination in the minimum possible time, becasue the reward of reaching target is same if the agent can reach the target before deadline. If we want to get the minimum possible time to reach the destination, it is better to add the relation between reward/penalty and time. \n",
    "\n",
    "3. The agent will fall into a weird trouble in very rare instances. It will stand in the same point for a long time or forever. The reason might be there is no penalty for time spending, one previous target location might have a high q-value for the next time, however the target changed in the next trails. The agent will chose the action with the highest q-value in this state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
