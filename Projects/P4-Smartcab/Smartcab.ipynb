{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Reinforcement Learning\n",
    "## Project 4: Smartcab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smartcab is a self-driving car from the not-so-distant future that ferries people from one arbitrary location to another. In this project, you will use reinforcement learning to train a smartcab how to drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a basic driving agent\n",
    "Implement the basic driving agent, which processes the following inputs at each time step:\n",
    "- Next waypoint location, relative to its current location and heading,\n",
    "- Intersection state (traffic light and presence of cars), and,\n",
    "- Current deadline value (time steps remaining),\n",
    "- Produces some random move/action (`None`, `forward`, `left`, `right`). \n",
    "\n",
    "Don’t try to implement the correct strategy! That’s exactly what your agent is supposed to learn.\n",
    "\n",
    "Run this agent within the simulation\n",
    "environment with enforce_deadline set to False (see run function in agent.py), and observe how it performs. In this mode, the agent is given unlimited time to reach the destination. The current state, action taken by your agent and reward/penalty earned are shown in the simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "*In your report, mention what you see in the agent’s behavior. Does it eventually make it to the target location?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  The agent moved randomly in each direction. It would reach the target location in the last, but the deadline was exceeded almost every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and update state\n",
    "Identify a set of states that you think are appropriate for modeling the driving agent. The main source of state variables are current inputs, but not all of them may be worth representing. Also, you can choose to explicitly define states, or use some combination (vector) of inputs as an implicit state.\n",
    "At each time step, process the inputs and update the current state. Run it again (and as often as you need) to observe how the reported state changes through the run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "*Justify why you picked these set of states, and how they model the agent and its environment.*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "\n",
    "The states I used are:\n",
    "- The traffic light `light`: ['red', 'green']\n",
    "- The traffic at the coming straight `oncoming`: [None, 'forward', 'left', 'right']\n",
    "- The traffic at the coming left `left`: [None, 'forward', 'left', 'right']\n",
    "- The traffic at the coming right `right`: [None, 'forward', 'left', 'right']\n",
    "- Next waypoint location `next_waypoint`: [None, 'forward', 'left', 'right']\n",
    "\n",
    "The goal of this project is to train the cab can reach the target and obey the traffic rules. \n",
    "\n",
    "The `light`, `oncoming`, `left`, and `right` states are related to traffic rules, the agent will get reward or penalty according to whether it obeyed the traffic rules. The traffic rules are:\n",
    "- `light`: ['red'], the agent can turn right if `oncoming` state is not ['left'] or `left` state is not ['forward']. \n",
    "- `light`: ['green'], the agent can turn left only if `oncoming` state is not ['forward'] .\n",
    "\n",
    "`next_waypoint` state is the best action calculated by route planner, the agent should obey the action from `nex_waypoint` as well to reach the target point as soon as possible.\n",
    "\n",
    "`deadline` is another state related with the reward. The agent can not get the final reward 10 if it can not reach the target within deadline time. However, this state will affact the performance when I added it into the set of sates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Q-Learning\n",
    "Implement the Q-Learning algorithm by initializing and updating a table/mapping of Q-values at each time step. Now, instead of randomly selecting an action, pick the best action available from the current state based on Q-values, and return that.\n",
    "Each action generates a corresponding numeric reward or penalty (which may be zero). Your agent should take this into account when updating Q-values. Run it again, and observe the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "*What changes do you notice in the agent’s behavior?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The agent moved randomly firstly, and then it obeyed the traffic rule, and followed the actions provided by `next_waypoint` state in the last.\n",
    "\n",
    "However, the agent can not reach the target within deadline time and won't obey the derection provided by route planner sometimes befor some parameters tuned(as shown in next question)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance the driving agent\n",
    "Apply the reinforcement learning techniques you have learnt, and tweak the parameters (e.g. learning rate, discount factor, action selection method, etc.), to improve the performance of your agent. Your goal is to get it to a point so that within 100 trials, the agent is able to learn a feasible policy - i.e. reach the destination within the allotted time, with net reward remaining positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "*Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "When the states only contain `light` and `next_waypoint` states:\n",
    "The success rate is 95/100\n",
    "the agent will stop at the \n",
    "when the learning rate less than 1, the success rate is not stable, the success rate would be very low and the time of breaking planner action would be high occasionally.\n",
    "<img src = 'images/results.tif'>\n",
    "- Action selection method is picking the available action with max Q-value as the best action from the current state based on Q-values.\n",
    "- Learning rate determines to what extent the newly acquired information will override the old information.\n",
    " - Learning rate is 0, it will make the agent not learn anything, the Q-values do not update.\n",
    " - Learning rate is 1, there is a very high success rate (0.98-1).\n",
    "- Discount factor determines the importance of future rewards. \n",
    " - It closes to 0 leads to \"myopic\" evaluation, while close to 1 leads to \"far-sighted\" evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "*Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "- There is a strong correlation between `Detergents_paper` and `Grocery` features. Moreover, `Milk` has some degree of correlation with `Fresh`, `Grocery`, and `Detergents_paper` features. \n",
    "- This does confirm my suspicions about the relevance of the feature I attempted to predict above.\n",
    "- All distributions of the data are skewed right. Most of the data distributed in left bottom of each figure. <img src = 'https://udacity-github-sync-content.s3.amazonaws.com/_imgs/29/1463303240/Screenshot_2016-05-15_at_11.06.51_AM.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
